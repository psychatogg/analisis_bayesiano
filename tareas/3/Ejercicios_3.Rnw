\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\allowdisplaybreaks


\begin{document}
<<include=FALSE,echo=FALSE>>=
options(width=60)
library(formatR)
@
Resolución \\
\\
1.1. \\
<<>>=
y <- c(2, 1, 4, 2, 3, 6, 2, 5, 4, 6)
x <- c(7, 6, 2, 8, 4, 1, 7, 3, 4, 2)

model_poiss <- glm(y ~ x, family = poisson)

summary(model_poiss)
@

1.2. \\
<<>>=
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
set.seed(1)

datos <- list(n = length(y), x = x, y = y)
codigo <- "
  data {
    int<lower=0> n;
    int<lower=0> y[n];
    int<lower=0> x[n];
  }
  
  parameters {
    real tau;
    real omega;
  }
  model {
  	real lambda;
    tau ~ normal(0, 0.5);
    omega ~ normal(0, 0.5);
    for (i in 1:n) {
    	lambda = exp(tau + omega*x[i]);
      y[i] ~ poisson(lambda);
    }
  }
"

fit <- stan(model_code = codigo, data = datos, iter = 1000)
print(fit)
@

1.3.\\
<<>>=
set.seed(1)

datos <- list(n = length(y), x = x, y = y)
codigo <- "
  data {
    int<lower=0> n;
    int<lower=0> y[n];
    int<lower=0> x[n];
  }
  
  parameters {
    real tau;
    real omega;
  }
  model {
  	real lambda;
    tau ~ normal(0, 5);
    omega ~ normal(0, 5);
    for (i in 1:n) {
    	lambda = exp(tau + omega*x[i]);
      y[i] ~ poisson(lambda);
    }
  }
"

fit <- stan(model_code = codigo, data = datos, iter = 1000)
print(fit)
@

1.4. \\
Hemos de tener en cuenta que el primer modelo es frecuentista y parte de diferentes supuestos y trata con la estimación puntual de los coeficientes; no considero que sus resultados sean comparables a los 2 modelos bayesianos siguientes, que tratan con distribuciones a posteriori. \\
Entre estos últimos, calculamos $AIC = 2 k -2 logverosimilitud$ : \\
<<>>=
AIC_2 <- 2*2 - 2*5.56
AIC_3 <- 2*2 - 2*11.08
AIC_2
AIC_3
@

Por lo tanto, determinamos que el modelo 3 presenta mejor ajuste teniendo en cuenta las penalizaciones de complejidad de dichos modelos.\\
Este a su vez nos indica que por cada hora de formación previa recibida, se cometen 0.2 errores menos de media, frente a los 0.07 del anterior. \\
\\
2.1. \\
<<>>=
set.seed(1)

x1 = c(28, 43, 19, 30, 25, 32, 45, 41, 26, 22, 32, 41, 36, 20, 29)
y = c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0)


datos <- list(n = length(y), x = x1, y = y)
codigo <- "
data {
  int<lower=0> n;
  int<lower=0,upper=1> y[n];
  real x[n];
}

parameters {
  real tau;
  real omega;
}

model {
	real pi; 
  tau ~ normal(0,5);
  omega ~ normal(0,5);
  for (i in 1:n) {
  	pi = (exp(tau+omega*x[i]))/(1+exp(tau+omega*x[i])) ;
    y[i] ~ bernoulli(pi);
  }
}
"

fit <- stan(model_code = codigo, data = datos, iter = 1000)
print(fit)

@

2.2. \\
Dado que el intervalo posterior de omega no incluye el 0, determinamos que la edad contribuye a la recuperación de forma significativa. Concretamente, disminuye la probabilidad de recuperación.\\ 
2.3.\\
<<>>=
set.seed(1)

x1 = c(28, 43, 19, 30, 25, 32, 45, 41, 26, 22, 32, 41, 36, 20, 29)
x2 = c(11, 17, 14, 12, 18, 12, 14, 16, 12, 13, 17, 15, 11, 13, 20)
y = c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0)

datos <- list(n = length(y), x1 = x1, x2 = x2, y = y)
codigo <- "
data {
  int<lower=0> n;
  int<lower=0,upper=1> y[n];
  real x1[n];
  real x2[n];
}

parameters {
  real tau;
  real omega1;
  real omega2;
}

model {
	real pi;
  tau ~ normal(0, 5);
  omega1 ~ normal(0, 5);
  omega2 ~ normal(0, 5);
  
  for (i in 1:n) {
  	pi = (exp(tau+omega1*x1[i]+omega2*x2[i]))/(1+exp(tau+omega1*x1[i]+omega2*x2[i]));
    y[i] ~ bernoulli(pi);
  }
}
"
fit <- stan(model_code = codigo, data = datos, iter = 1000)
print(fit)
@

2.4.\\
Asumiendo que no existe interacción entre edad y puntuación en depresión, concluimos que, al incluir ambos intervalos posteriores el 0, contamos con incertidumbre en lo que a la contribución de cada variable predictora a la predicción de recuperación se refiere. \\
Interpretando los efectos principales, encontramos que el aumento en la edad disminuye las probabilidades de recuperación, al igual que ocurre con el aumento de la puntuación en depresión.
\end{document}